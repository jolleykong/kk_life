Executive Summary

=================

 

WL#6314 implemented intra-schema multi-threaded slave, where multiple transactions can be applied in parallel on the slave (even when they modify same database) as long as they are non-conflicting.

 

The existing implementations uses a simple algorithm for determining which transactions are non-conflicting. It already gives a 40% improvement on one benchmark, compared to single-threaded slave.

 

The present worklog optimizes the implementation by using a potentially more precise algorithm to determine which transactions are non-conflicting. On workloads that are not evenly distributed on many databases, this allows more transactions to execute in parallel.

 

requirements

=================

After this worklog the Intra-schema multi-threaded slave should be optimized and we should be able to gain more throughput.

 

No change to server user interface. The new scheme will completely replace the scheme implemented in WL#6314.

 

FR1: Additional fields will be added to the binary log.

 

FR2: mysqlbinlog output shall show the two logical timestamps for all transactions.

 

NFR1: Some transactions may be tagged with zero values of last_committed and sequence_number. In this case transaction's scheduling on slave reduces parallelism. It's insignificant penalty as long as the number of such transactions is small.

 

NFR2: Some transactions/event groups are tagged as dependent on a preceding one even though the actual dependency is different (more relaxing). It's insignificant penalty as long as the number of such transactions is small.

 

 

 

Old, Commit-Parent-Based Scheme

===============================

 

The old scheme for multi-threaded slave implemented in WL#6314 works as follows.

 

\- On master, there is a global counter. The counter is incremented before each storage engine commit.

 

\- On master, before a transaction enters the prepare phase, the current value of the global counter is stored in the  transaction. This number is called the commit-parent for the  transaction.

 

\- On master, the commit-parent is stored in the binary log in the header of the transaction.

 

\- On slave, two transactions are allowed to execute in parallel if they have the same commit-parent.

 

Problem With Commit-Parent-Based Scheme

=======================================

 

The old scheme allows less parallelism than would be possible.

 

The old scheme partitions the time line into intervals. When a transaction commits, the current time interval ends and a new begins. 

Two transactions can execute in parallel if they were prepared within the same time interval. The following picture illustrates the scheme:

 

```
  Trx1 ------------P----------C-------------------------------->
                |
  Trx2 ----------------P------+---C---------------------------->
                |  |
  Trx3 -------------------P---+---+-----C---------------------->
                |  |   |
  Trx4 -----------------------+-P-+-----+----C----------------->
                |  |   |  |
  Trx5 -----------------------+---+-P---+----+---C------------->
                |  |   |  |  |
  Trx6 -----------------------+---+---P-+----+---+---C---------->
                |  |   |  |  |  |
  Trx7 -----------------------+---+-----+----+---+-P-+--C------->
                |  |   |  |  |  | |
```

 

Each horizontal line represents a transaction. Time progresses to the right. P denotes the point in time when the commit-parent is read before the prepare phase. C denotes the point in time when the transaction increases the global counter and thus begins a new interval. The vertical lines extending down from each commit show the interval boundaries. 

 

Trx5 and Trx6 are allowed to execute in parallel because they have the same commit-parent (namely, the counter value set by Trx2). However, Trx4 and Trx5 are not allowed to execute in parallel, and Trx6 and Trx7 are not allowed to execute in parallel.

 

But note that two transactions that hold all their respective locks at the same point in time on the master are necessarily non-conflicting. 

Thus, it would not be problematic to allow them to execute in parallel on the slave. In the above example, this has two implications:

 

\- Trx4, Trx5, and Trx6 hold all their locks at the same time but Trx4 will be executed in isolation.

 

\- Trx6 and Trx7 hold all their locks at the same time but Trx7 will be executed in isolation.

 

It would be better if Trx4 could execute in parallel with Trx5 and Trx6, and Trx6 could execute in parallel with Trx7.

 

New, Lock-Based Scheme

======================

 

In the present worklog we implement a scheme that allows two transactions to execute in parallel if they hold all their locks at the same time.

 

We define the lock interval as the interval of time when a transaction holds all its locks:

 

\- The lock interval ends when the first lock is released in the storage engine commit. For simplicity, we do not analyze the lock releases inside the storage engine; instead, we assume that locks are released just before the storage engine commit.

 

\- The lock interval begins when the last lock is acquired. This may happen in the storage engine or in the server. For simplicity, we do not analyze lock acquisition in the storage engine or in the server; instead, we assume that the last lock is acquired at the end of the last DML statement, in binlog_prepare. This works correctly both for normal transactions and for autocommitted transactions.

 

If Trx1, Trx2 are transactions, and Trx1 appears before Trx2, the criterion for parallel execution is this:

 

C1. Trx1, Trx2 can execute in parallel if and only if their locking intervals overlap.

 

The following is an equivalent formulation:

 

C2. Trx1, Trx2 can NOT execute in parallel, if and only if Trx1 has ended its locking interval before Trx2 has started its locking interval.

 

The following illustrates the criteria (L denotes the beginning of the locking interval and C denotes the end of the locking interval).

 

 \- Can execute in parallel:

  Trx1 -----L---------C------------>

  Trx2 ----------L---------C------->

 

 \- Can not execute in parallel:

  Trx1 -----L----C----------------->

  Trx2 ---------------L----C------->

 

To evaluate the locking criteria, we need to keep track of which transactions have ended their locking intervals. To this end, we assign a logical timestamp to each transaction:

transaction.sequence_number. We will need to store transaction.sequence_number in the binary log. Therefore, we step it and assign it to the transaction just before the transaction enters the flush stage.

 

In addition, we maintain the global variable global.max_committed_transaction, which holds the maximal sequence_number of all transactions that have ended their locking intervals. The variable plays a role of the system commit logical clock. Thus, before a transaction performs storage engine commit, it sets global.max_committed_transaction to max(global.max_committed_timestamp, transaction.sequence_number).

 

Each transaction needs to know which transactions it cannot execute in parallel with. We define the *commit parent* of a transaction to be the *newest* transaction that cannot execute in parallel with the transaction. Thus, when the transaction begins its locking interval, we store global.max_committed_timestamp into the variable transaction.last_committed. Recall that the locking interval for multi-statement transactions begins at the end of the last statement before commit. Since we do not know a priori which is the last statement, we store global.max_committed_timestamp into transaction.last_committed at the end of *every* DML statement, overwriting the old value. Then we will have the correct value when the transaction is written to the binary log.

 

We store both timestamps in the binary log.

 

The condition for executing a transaction on the slave is as follows:

 

C3. Slave can execute a transaction if the smallest sequence_number among all executing transactions is greater than transaction.last_committed.

 

In order to check this condition, the slave scheduler maintains an ordered sequence of currently executing transactions. The first transaction in the sequence is the one that appeared first in the master binary log. In other words, it is the one with the smallest value for transaction.sequence_number. The last transaction in the sequence is the one that appeared last in the master binary log, i.e., has the greatest value for transaction.transaction_counter 

 

Before a transaction is taken for scheduling, the following condition is

checked:

 

 (*) transaction_sequence[0].sequence_number > this.last_committed

 

Scheduling holds up until this condition becomes true. At successful scheduling, the transaction is appended at the end of transaction_sequence.

 

After a transaction has committed, it is effectively removed from the sequence. (In the implementation, it is merely marked as done, which tells the scheduler to ignore the transaction when it evaluates condition (*)).

 

 

Pseudo-code

===========

 

Master variables:

\- int64 global.transaction_counter

\- int64 global.max_committed_transaction

\- int64 transaction.sequence_number

\- int64 transaction.last_committed

 

Master logic in order of events of execution:

 

\- in binlog_prepare:

 

  if this is not a transaction commit:

   transaction.last_committed = global.max_committed_transaction

 

\- after it has been determined that the transaction is the next one to  be flushed, and before transaction is flushed, the global transaction  counter is stepped and copied to the transaction's sequence number:

 

  transaction.sequence_number = ++global.transaction_counter

 

\- write transaction.sequence_number and transaction.last_committed to the binary log, in the transaction header;

 

\- before transaction does storage engine commit:

 

  global.max_committed_transaction = max(global.max_committed_transaction,

​                      transaction.sequence_number)

 

 

 When @@global.binlog_order_commits is true, in principle we could reduce the max to an assignment:

 

  global.max_committed_transaction = transaction.sequence_number

 

 However, since binlog_order_commits is dynamic, if we do this, there will  be a short time interval just after user change binlog_order_commits from  0 to 1, during which the committing transactions' timestamps are not  monotonically increasing, but binlog_order_commits == 1. If we used the  assignment algorithm during this time period, transactions could have the  wrong timestamps in the binary log, which could lead to conflicting  transactions executing in parallel on the slave.

 

 To handle both cases using atomic operations we use the following algorithm:

 

 int64 old_value = transaction.sequence_number - 1;

 while (!my_atomic_cas64(&global.max_committed_transaction,

​             &old_value, transaction.sequence_number) &&

​     transaction.sequence_number > old_value)

   ; // do nothing

 

Slave variables:

 

\- transaction_sequence: ordered sequence containing all executing transactions in order of increasing sequence_number.

 

 (In the code, this is implemented using the existing Relay_log_info::GAQ. This is a circular queue of large, fixed size.)

 

Slave logic:

 

\- before scheduler pushes the transaction for execution:

 

  wait until transaction_sequence[0].sequence_number >

​        transaction.last_committed

 

 (The actual implementation will step through the list in the following manner:

 

  // The Low Water Mark is the newest transaction for which the scheduler

  // knows the following facts:

  // - the transaction has been committed;

  // - all older transactions have been committed.

  // LWM_plus_1 is the next transaction, i.e., the one that was the oldest

  // executing transaction last time that the schedule looked.

 

  global int LWM_plus_1; // the same as transaction_sequence[0]

 

  function wait_until_transaction_can_be_scheduled(transaction):

   while true:

​    while rli.GAQ[LWM_plus_1].is_committed:

​     LWM_plus_1++

​    if rli.GAQ[LWM_plus_1].sequence_number > transaction.last_committed:

​     return

​    wait until rli.GQA[LWM_plus_1] commits

 

\- after transaction commits:

 GAQ[transaction.index].is_committed = true;

 

Corner cases

============

 

 \1. Handle exhaustion of the counters. (Note, this will never happen,   because it takes 500 years to wrap a 64 bit counter if you have   1,000,000,000 transactions per second, but we should handle it   because people usually worry about such things.) 

 

  If the counter wraps, we should rotate the binary log. The slave coordinator should make a checkpoint and wait for all currently running threads when it sees a rotate. This mechanism is already implemented for the current scheme, so all we need is a test case.

 

 \2. Fall back to sequential execution.

  In certain cases a transaction is not scheduled in parallel to require all prior to have been finished (a similar policy exists in WL#5569). Transaction header event is tagged with last_committed value of zero, and possibly with last_committed of zero. Those rare cases include:

 

  \- "old" WL7165 unaware master transaction, incl wl6134-aware ones

  \- DROP of multiple tables is logged such way with a second Query event

  \- CREATE table ... SELECT ... from @user-var, or rand function, or

   INTVAR is generated for the query.

 

 \3. Mixed engine transaction is logged as multiple (two) groups, where

   the 2nd is tagged to have the 1st as its commit parent.

 

Optimizations

=============

 

 \1. Access to global.transaction_counter does not need a lock because flushes are serialized on the master.

 

 \2. The two numbers stored in the binary log will normally have a very small difference. So instead of storing two 64-bit integers, we can store transaction.sequence_number as a 64-bit integer, and then store the difference as a 16-bit integer. This will save 6 bytes of space. In the case that the difference is greater than 65535, we store the number 65535. This is safe, but may give less parallelism (in the case of 65536 or more concurrent transactions on the master).

 

 

==== Notes for future work ====

 

  The ideas of this section are *not* to be included in this worklog;

  they are merely mentioned here to prevent possible concerns and motivate the current design choice.

 

 \1. Logical timestamp compressing in the binlog event

  

  If binlog_order_commits=OFF, the current policy of storing just two numbers in the binary log may give sub-optimal scheduling on the slave. This could in theory be fixed by replacing transaction.last_committed by a more complex data structure. However, this would be both more conceptually complex and require a more complex implementation, as well as more data in the binary log. It also only addresses a corner case (the default is binlog_order_commits=ON and there is no known reason to turn it off). Therefore, we do not intend to fix that in this worklog.

 

  Just for the record, we here outline the problem and a possible solution; this may be considered future work in case it is determined to be useful.

 

  The precise problem is that when binlog_order_commits=OFF, it is possible for two transactions to be committed in a different order than the order in which they were flushed. Thus, even if trx1 is written before trx2 to the binary log and thus trx1.sequence_number < trx2.sequence_number, it is possible that trx2 is committed before trx1. This gives the following possible scenario of sub-optimal scheduling on the slave:

 

   \1. T1 flushes and is assigned transaction.sequence_number := 1

   \2. T2 flushes and is assigned transaction.sequence_number := 2

   \3. T2 commits and sets global.max_committed_transaction := 2

   \4. T3 reads transaction.last_committed :=

​    global.max_committed_transaction = 2

   \5. T1 commits

 

  Then, the slave will not schedule T3 at the same time as T1 or T2. However, it would have been ok for T3 to execute in parallel with T1, since T1 held all locks at the same time as T3.

 

  To fix this, we would need to replace transaction.last_committed by the set of all sequence numbers that have not yet been committed. Currently, we know that this set only contains consecutive values, so it is conveniently stored as a single integer, but when commits may happen in a different order from the assignment of transaction.sequence_number, the set may be more complex. The set of sequence numbers that have not been committed can then be represented as a list of intervals, or as an offset plus a bitmap (if bit number N is set in the bitmap, it means that sequence number offset+N has been generated but not committed).

 

 \2. Transaction distribution policies

 

  Among substantial factors to consider there's the style of the jobs assigning (feeding) to Workers. There are two being considered for the immediate evaluation, yet only the first one (A) implemented and is present in this section to contrast with the 2nd (B).

 

  A. At-Most-One (which had been designed yet by WL#6314) Any worker can have only at most one transaction in its private queue. In case all workers are occupied, which is actually expected 'cos the read time is about 1% of execution time, the Coordinator gets to waiting for release of any first of them. Potential disadvantage is apparent, in the worst case all but one Worker can be without any assignment for duration of scheduling of few transactions. And it actually scales up: the last of the idling workers would experience hungry time for duration of N-1 scheduling times.

 

  B. The First Available (arguably ideal, not to be implemented in this WL) The idea is use a shared queue to hold the transaction events that Coordinator pushes into, and Worker pick up from the (other) end. Such queue design had been done ago at the DB-type "classing" MTS. The queue features concurrent access (push and pop) by multiple threads.

执行摘要

=================

 

WL＃6314实现了架构内多线程从属服务器，只要不冲突，就可以在该从属服务器上并行应用多个事务（即使它们修改了相同的数据库）。

 

现有的实现使用一种简单的算法来确定哪些事务没有冲突。 与单线程从属服务器相比，它已经在一个基准上提高了40％。

 

本工作日志通过使用可能更精确的算法来确定哪些事务没有冲突来优化实现。 对于未在许多数据库上平均分配的工作负载，这允许更多事务并行执行。

 

 

 

要求

=================

完成此工作日志后，应该优化架构内多线程从属服务器，并且我们应该能够获得更多的吞吐量。

 

无需更改服务器用户界面。 新方案将完全替代WL＃6314中实现的方案。

 

FR1：其他字段将添加到二进制日志中。

 

FR2：mysqlbinlog输出应显示所有事务的两个逻辑时间戳。

 

NFR1：某些事务可能被标记为last_committed和sequence_number的零值。 在这种情况下，事务在从属服务器上的调度减少了并行性。 只要此类交易的数量很少，这是微不足道的罚款。

 

NFR2：即使实际的依赖关系不同（更轻松），某些事务/事件组也被标记为依赖于先前的事务/事件。 只要此类交易的数量很少，这是微不足道的罚款。

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

基于锁定的新方案

======================

 

在本工作日志中，我们实现了一个方案，如果两个事务同时持有所有锁，则该事务允许两个事务并行执行。

 

我们将锁定间隔定义为事务持有所有锁定的时间间隔：

 

-当存储引擎提交中的第一个锁释放时，锁定间隔结束。为简单起见，我们不分析存储引擎内部的锁释放；相反，我们假定在存储引擎提交之前释放了锁。

 

-锁定间隔在获取最后一个锁定时开始。这可能在存储引擎或服务器中发生。为简单起见，我们不分析存储引擎或服务器中的锁获取；相反，我们假定在binlog_prepare中最后一个DML语句的末尾获取了最后一个锁。这对于正常事务和自动提交的事务均正确工作。

 

如果Trx1，Trx2是事务，并且Trx1出现在Trx2之前，则并行执行的条件是：

 

C1。且只有当它们的锁定间隔重叠时，Trx1，Trx2才能并行执行。

 

以下是等效公式：

 

C2。且只有当Trx1在Trx2开始其锁定间隔之前已结束其锁定间隔时，Trx1，Trx2才能并行执行。

 

下面说明了标准（L表示锁定间隔的开始，C表示锁定间隔的结束）。

 \- Can execute in parallel:

  Trx1 -----L---------C------------>

  Trx2 ----------L---------C------->

 

 \- Can not execute in parallel:

  Trx1 -----L----C----------------->

  Trx2 ---------------L----C------->

 

要评估锁定标准，我们需要跟踪哪些事务已结束其锁定间隔。为此，我们为每个事务分配一个逻辑时间戳：

transaction.sequence_number。我们将需要在二进制日志中存储transaction.sequence_number。因此，我们在事务进入flush阶段之前将其步进并分配给事务。

 

此外，我们维护全局变量global.max_committed_transaction，该变量保存所有已结束锁定间隔的事务的最大sequence_number。该变量起系统提交逻辑时钟的作用。因此，在事务执行存储引擎提交之前，它会将global.max_committed_transaction设置为max（global.max_committed_timestamp，transaction.sequence_number）。

 

每个事务都需要知道它不能并行执行的事务。我们将事务的“提交父”定义为不能与该事务并行执行的“最新”事务。因此，当事务开始其锁定间隔时，我们将global.max_committed_timestamp存储到变量transaction.last_committed中。回想一下，多语句事务的锁定间隔从提交之前的最后一条语句的末尾开始。由于我们不知道最后一条语句是先验的，因此我们在* every * DML语句的末尾将global.max_committed_timestamp存储到transaction.last_committed中，从而覆盖旧值。然后，当将事务写入二进制日志时，我们将获得正确的值。

 

我们将两个时间戳存储在二进制日志中。

 

在从站上执行事务的条件如下：

 

C3。如果所有正在执行的事务中最小的sequence_number大于transaction.last_committed，则从设备可以执行事务。

 

为了检查此条件，从属调度程序维护当前执行事务的有序序列。序列中的第一个事务是在主二进制日志中首先出现的事务。换句话说，它是transaction.sequence_number值最小的那个。序列中的最后一个事务是在主二进制日志中最后出现的事务，即，具有最大事务值的事务。

 

在进行事务进行调度之前，以下条件是

已检查：

 

 （*）transaction_sequence [0] .sequence_number> this.last_committed

 

调度一直持续到该条件变为真。在成功调度后，事务将附加在transaction_sequence的末尾。

 

提交事务后，将其有效地从序列中删除。 （在实现中，它仅标记为完成，它告诉调度程序在评估条件（*）时忽略该事务）。

 

 

伪码

===========

 

主变量：

-int64 global.transaction_counter

-int64 global.max_committed_transaction

-int64 transaction.sequence_number

-int64 transaction.last_committed

 

按照执行事件的顺序掌握逻辑：

 

-在binlog_prepare中：

 

  如果这不是事务提交：

   transaction.last_committed = global.max_committed_transaction

 

-在确定事务是要刷新的下一个事务之后，并且在刷新事务之前，将全局事务计数器步进并复制到事务的序列号：

 

  transaction.sequence_number = ++ global.transaction_counter

 

-在交易标头中将transaction.sequence_number和transaction.last_committed写入二进制日志；

 

-在事务执行存储引擎提交之前：

 

  global.max_committed_transaction = max（global.max_committed_transaction，

​                      transaction.sequence_number）

 

 

 当@@ global.binlog_order_commits为true时，原则上我们可以将max减小为一个赋值：

 

  global.max_committed_transaction = transaction.sequence_number

 

 但是，由于binlog_order_commits是动态的，因此如果执行此操作，则在用户将binlog_order_commits从0更改为1之后，将有一个较短的时间间隔，在此期间提交事务的时间戳不会单调增加，而binlog_order_commits == 1。在此时间段内使用分配算法，事务可能在二进制日志中具有错误的时间戳，这可能导致冲突的事务在从属服务器上并行执行。

 

 为了使用原子操作处理两种情况，我们使用以下算法：

 

 int64 old_value = transaction.sequence_number-1;

 而（！my_atomic_cas64（＆global.max_committed_transaction，

​             ＆old_value，transaction.sequence_number）&&

​     transaction.sequence_number> old_value）

   ; // 没做什么

 

从属变量：

 

-transaction_sequence：包含所有正在执行的事务的有序序列，其顺序为sequence_number递增。

 

 （在代码中，这是使用现有的Relay_log_info :: GAQ实现的。这是一个固定大小的循环队列。）

 

从站逻辑：

 

-在调度程序推送事务以执行之前：

 

  等到transaction_sequence [0] .sequence_number>

​        transaction.last_committed

 

 （实际实现将以以下方式逐步遍历列表：

 

  //低水位标记是调度程序的最新事务

  //知道以下事实：

  //-交易已提交；

  //-所有较旧的事务均已提交。

  // LWM_plus_1是下一个交易，即最早的交易

  //上次查看时间表时执行交易。

 

  全局int LWM_plus_1; //与transaction_sequence [0]相同

 

  函数wait_until_transaction_can_be_scheduled（transaction）：

   虽然为真：

​    而rli.GAQ [LWM_plus_1] .is_committed：

​     LWM_plus_1 ++

​    如果rli.GAQ [LWM_plus_1] .sequence_number> transaction.last_committed：

​     返回

​    等到rli.GQA [LWM_plus_1]提交

 

-交易提交后：

 GAQ [transaction.index] .is_committed = true;

 

 

角落案例

============

 

 1.处理计数器用尽的情况。 （请注意，这永远不会发生，因为如果您每秒有1,000,000,000个事务，那么要花500年的时间包装64位计数器，但是我们应该处理它，因为人们通常会担心这种事情。）

 

  如果计数器回绕，则应旋转二进制日志。从属协调器应该进行检查，并在看到旋转时等待所有当前正在运行的线程。当前方案已经实现了该机制，因此我们只需要一个测试用例。

 

 2.退回到顺序执行。

  在某些情况下，未并行调度事务以要求所有事务都已完成（WL＃5569中存在类似的策略）。事务标头事件被标记为last_committed值为零，并且可能标记为last_committed为零。这些罕见的情况包括：

 

  -“旧” WL7165不知道主事务，包括wl6134-知道的事务

  -使用第二个查询事件以这种方式记录多个表的DROP

  -CREATE table ... SELECT ... from @ user-var或rand函数，或者

   为查询生成INTVAR。

 

 3.混合引擎事务记录为多个（两个）组，其中

   第二个被标记为将第一个作为其提交父对象。

 

最佳化

=============

 

 1.对global.transaction_counter的访问不需要锁，因为刷新是在主服务器上序列化的。

 

 2.二进制日志中存储的两个数字通常会有很小的差异。因此，除了存储两个64位整数之外，我们可以将transaction.sequence_number存储为64位整数，然后将差值存储为16位整数。这将节省6个字节的空间。在差异大于65535的情况下，我们将存储数字65535。这是安全的，但可能会减少并行性（在65536或主服务器上有更多并发事务的情况下）。

 

 

====未来工作的注意事项====

 

  本节的思想*不*包括在本工作日志中；

  在此仅提及它们是为了避免可能的担忧并激发当前的设计选择。

 

 \1. binlog事件中的逻辑时间戳压缩

  

  如果binlog_order_commits = OFF，则当前在二进制日志中仅存储两个数字的策略可能会在从属服务器上提供次优的调度。从理论上讲，这可以通过用更复杂的数据结构替换transaction.last_committed来解决。但是，这将在概念上更加复杂并且需要更复杂的实现，以及二进制日志中的更多数据。它还仅处理一个极端情况（默认值为binlog_order_commits = ON，并且没有已知的理由将其关闭）。因此，我们不打算在此工作日志中解决此问题。

 

  仅作记录，我们在此概述问题和可能的解决方案。如果确定有用，则可以将其视为将来的工作。

 

  确切的问题是，当binlog_order_commits = OFF时，两个事务可能以与刷新顺序不同的顺序提交。因此，即使将trx1在trx2之前写入二进制日志并因此trx1.sequence_number <trx2.sequence_number，也可能在trx1之前提交trx2。这给出了从属服务器上次优调度的以下可能情况：

 

   \1. T1刷新并被分配transaction.sequence_number：= 1

   \2. T2刷新并分配了transaction.sequence_number：= 2

   \3. T2提交并设置global.max_committed_transaction：= 2

   \4. T3读取transaction.last_committed：=

​    global.max_committed_transaction = 2

   \5. T1提交

 

  这样，从站将不会与T1或T2同时调度T3。但是，T3与T1并行执行是可以的，因为T1与T3同时持有所有锁。

 

  要解决此问题，我们需要用尚未提交的所有序列号的集合替换transaction.last_committed。当前，我们知道此集合仅包含连续的值，因此可以方便地将其存储为单个整数，但是当提交可能以与transaction.sequence_number的分配不同的顺序发生时，该集合可能会更复杂。然后可以将未提交的序列号集合表示为间隔列表，或表示为偏移量加上位图（如果在位图中设置了位号N，则表示已经生成了序列号offset + N，但是未提交）。

 

 

2.交易分配政策

 

  要考虑的重要因素包括分配（供餐）给工人的工作风格。立即评估有两个考虑因素，但只有第一个（A）实现了，并且在本节中与第二个（B）形成对比。

 

  答：最多一（由WL＃6314设计）任何工作者在其专用队列中最多只能有一个事务。如果所有工作人员都被占用，这实际上是预期的，因为读取时间约为执行时间的1％，那么协调器将开始等待释放其中的任何一个。潜在的缺点是显而易见的，在最坏的情况下，除了少数一个工作人员之外，所有工作人员都可以在很少的事务调度期间进行任何分配。它实际上在扩大：最后一个空转的工人将经历N-1个调度时间的饥饿时间。

 

  B. First Available（可以说是理想的，在本WL中不实现）的想法是使用共享队列来保存协调器推送到的事务事件，而Worker从（另一端）提取。这种队列设计是在DB类型的“分类” MTS之前完成的。队列具有多个线程的并发访问（推送和弹出）功能。

 